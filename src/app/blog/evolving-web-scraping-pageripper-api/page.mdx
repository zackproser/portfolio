import { ArticleLayout } from '@/components/ArticleLayout'
import { Button } from '@/components/Button'
import Image from 'next/image'

import pageripperBot from '@/images/pageripper-javascript.png'
import serverSideScraping from '@/images/serverside-scraping.png'
import clientSideScraping from '@/images/clientside-scraping.png'
import pageripperPuppeteer from '@/images/pageripper-puppeteer.png'

import { generateOgUrl } from '@/utils/ogUrl'

export const meta = {
    author: "Zachary Proser", 
    date: "2023-12-28",
    title: "Evolving web scraping: How Pageripper API handles JavaScript-heavy sites", 
    description: "Pageripper API uses Puppeteer and headless Chrome under the hood to see the same thing your browser does, even for Single Page Applications (SPAs)",
    image: pageripperBot
}

export const metadata = {
  openGraph: {
    title: meta.title,
    description: meta.description,
    url: "${process.env.NEXT_PUBLIC_SITE_URL}", 
    siteName: "Zack Proser portfolio",
    images: [
      {
        url: `${generateOgUrl(meta)}`,
      }
    ]
  }
}

export default (props) => <ArticleLayout meta={meta} {...props} />

--- 

<Image src={pageripperBot} alt="Pageripper API a commercial data-extraction service" />

## Introduction

[Pageripper is a commercial API](https://rapidapi.com/zackproser/api/pageripper/) that extracts data from webpages, even if they're rendered with Javascript. 

## Challenges in Scraping JavaScript-Heavy Sites

Scraping data and content from sites that rely on client-side Javascript for rendering requires a different approach from scraping sites that render their content 
completely on the server. 

These two flow diagrams illustrate the difference: when you're scraping a server-side rendered site, the initial page you fetch tends to already include all rendered content.

### Scraping a server-side rendered site 

There's not much to do after we receive the requested page, since we can access whatever CSS selectors or elements we want - they're all already rendered.

<Image src={serverSideScraping} alt="In many ways, scraping a server-side rendered site is simpler" />

### Scraping a client-side rendered site

However, when we're dealing with a client-side rendered (CSR) site, we need to employ a browser or Javascript engine to run the code bundle responsible for rendering elements...

<Image src={clientSideScraping} alt="Scraping from a Javascript-heavy site is more involved" />

### Pageripperâ€™s Approach

Pageripper handles both the client-side and server-side rendered sites by employing Puppeteer under the hood. Puppeteer is project that simplifies running a headless browser in Node.js. 

<Image src={pageripperPuppeteer} alt="Pageripper uses Puppeteer to handle client-side rendered sites" />

In addition, Pageripper allows users to tweak the behavior of Puppeteer on a per-request basis. This means that you can specify which network event to wait on when scraping a given site. 

For example, if you're using Pageripper to scrape a server-side rendered (SSR) site, you can likely get away with passing a `waitUntilEvent` parameter of `domcontentloaded`, meaning that Puppeteer will only 
wait until the DOM content is loaded before scraping. This is noticeably faster most of the time. 

However, when you're dealing with a Javascript-heavy or Client-side rendered site, you could pass `waitUntilEvent` as `networkidle0` to wait until the network is idle (there are no resources left to download) before rendering and scraping the content.

```bash
```

### Future Roadmap

Today, Pageripper automatically categorizes links (internal vs external), ecommerce links, asset download links, etc. It also extracts email addresses and Twitter handles. 

Over time, Pageripper will be expanded to extract even more data from a given page

Conclusion: Reinforce the current and future value of Pageripper in the web scraping domain.
