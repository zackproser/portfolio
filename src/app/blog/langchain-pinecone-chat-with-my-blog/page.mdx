import Image from 'next/image';
import Link from 'next/link';

import customRagChat from '@/images/custom-rag-chat-screenshot.webp';
import customRagFlowchart from '@/images/chat-with-blog-flowchart.webp';
import googleColabSecrets from '@/images/rag-tutorial-colab-secrets.webp';

import { createMetadata } from '@/utils/createMetadata';
import { ArticleLayout } from '@/components/ArticleLayout';

export const articleMetadata = {
    author: "Zachary Proser", 
    date: "2024-05-10",
    title: "Build a RAG pipeline for your blog with LangChain, OpenAI and Pinecone",
    description: "Learn how to build a production-ready RAG pipeline that lets visitors chat with your content, complete with citations and related content suggestions",
    image: customRagChat,
    slug: 'langchain-pinecone-chat-with-my-blog',
    isPaid: true,
    price: 2000,
    previewLength: 450,
    previewElements: 37,
    paywallHeader: "Building RAG Pipelines: The Most In-Demand Gen AI Skill",
    paywallBody: "Every company wants developers who can build RAG applications. Get a complete, production-ready tutorial that teaches you exactly how to build a RAG pipeline with the latest tech stack: Vercel AI SDK, OpenAI embeddings, and Pinecone vector search.",
    buttonText: "Unlock the full tutorial ($20)"
};

export const metadata = createMetadata(articleMetadata);

export default (props) => <ArticleLayout metadata={articleMetadata} {...props} />

This tutorial contains everything you need to build production-ready Retrieval Augmented Generation (RAG) pipelines on your own data. Whether you're working with a corporate knowledge base, personal blog, or ticketing system, you'll learn how to create an AI-powered chat interface that provides accurate answers with citations.

## Try It Yourself

See the complete working demo at [/chat](/chat). This tutorial walks you through building this exact same experience:

<Link href="/chat">
<Image src={customRagChat} alt="Chat interface with related posts suggestions" />
</Link>

<iframe 
  className="youtube-video"
  src="https://www.youtube.com/embed/ii4aUE-6Okk" 
  title="Building an AI chatbot with LangChain, Pinecone.io and OpenAI" 
  frameBorder="0" 
  allow="fullscreen;">
</iframe>

## Skills You'll Learn

- Data preprocessing and preparation with LangChain
- Converting text to embeddings with OpenAI
- Creating and managing vector databases with Pinecone
- Implementing query embeddings and vector similarity search
- Enriching LLM prompts with private context at runtime
- Building streaming chat interfaces with Vercel AI SDK
- Adding citations and related content suggestions

## What We're Building

Let's understand the complete system we'll be creating:

<Image src={customRagFlowchart} alt="RAG Pipeline Architecture" />

This is a Retrieval Augmented Generation (RAG) pipeline that allows users to chat with your content. Here's how it works:

1. When a user asks a question, their query is converted to a vector (embedding)
2. This vector is used to search your Pinecone database for similar content
3. The most relevant content is retrieved and injected into the LLM's prompt
4. The LLM generates a response based on your content
5. The response is streamed back to the user along with citations

We'll build this system in the following order:

1. **Data Processing**: First, we'll process your content (blog posts, documentation, etc.) into a format suitable for vector search. We'll use a Jupyter Notebook for this phase.

2. **Vector Database Creation**: We'll convert your processed content into embeddings and store them in Pinecone, creating a searchable knowledge base.

3. **Knowledge Base Testing**: We'll verify our setup by running semantic search queries against the vector database to ensure we get relevant results.

4. **Backend Development**: We'll build the Next.js API that:
   - Accepts user queries
   - Converts queries to embeddings
   - Retrieves relevant content from Pinecone
   - Provides context to the LLM
   - Streams the response back to the user

5. **Frontend Implementation**: Finally, we'll create the chat interface that:
   - Accepts user input
   - Makes API calls to our backend
   - Displays streaming responses
   - Shows related content and citations

Let's get started with the implementation.

## Detailed Implementation Guide

### Step 1: Setting Up Your Knowledge Store

I've created a [Jupyter Notebook](https://github.com/zackproser/ingest-portfolio/blob/main/ingest_portfolio.ipynb) that handles all the data preprocessing and vector database creation. This notebook is designed to be easily customizable - you can swap out my example site with your own content source in just a few lines of code.

#### Setting up Google Colab

1. First, open the [notebook in Google Colab with this direct link](https://colab.research.google.com/github/zackproser/ingest-portfolio/blob/main/ingest_portfolio.ipynb):

*google colab import notebook image*

2. Configure your API keys in Colab's secrets manager:

* Obtain your OpenAI API key from [here](https://platform.openai.com/api-keys)
* Obtain your Pinecone API key from [here](https://app.pinecone.io/organizations/-/projects/-/keys)

Name your OpenAI API key secret `OPENAI_API_KEY` your Pinecone API key secret `PINECONE_API_KEY`.

Ensure the **Notebook access** toggles are enabled for both secrets. This grants your notebook access to the secrets.

<Image src={googleColabSecrets} alt="Google Colab Secrets" />

```python
# Access your secrets safely in your notebook
from google.colab import userdata
os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')
os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')
```

#### Understanding Key Components

Let's walk through the important parts of the notebook:

1. **Content Loading**: This cell loads your content into memory:
```python
from langchain_community.document_loaders import DirectoryLoader

# Configure the loader for your content
loader = DirectoryLoader(
    'your-content-dir',           # Replace with your content directory
    glob="**/*.mdx",             # Adjust pattern to match your files
    show_progress=True
)
docs = loader.load()

print(f"Loaded {len(docs)} documents")
```

The `DirectoryLoader` recursively finds all matching files in your directory. Change the `glob` pattern to match your content files (e.g., `"**/*.txt"` for text files).

2. **Content Chunking**: This cell splits your content into optimal chunks for embedding:
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Create chunks that balance size and context
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,              # Smaller chunks for precise retrieval
    chunk_overlap=50,            # Overlap to maintain context
    length_function=len,
    separators=["\n\n", "\n", " ", ""]  # Try to split on natural boundaries
)
split_docs = text_splitter.split_documents(docs)

print(f"Created {len(split_docs)} chunks")
```

The `chunk_size` and `chunk_overlap` parameters are crucial for retrieval quality:
- Smaller chunks (500-1000 characters) work well for precise answers
- Overlap ensures we don't break context at chunk boundaries

3. **Understanding Embeddings**: 

Embeddings or vectors are lists of floating point numbers that represent semantic meaning and relationships between entities in data. When a user asks a question, their query is converted into a vector that looks like this:

```bash
lastMessage: { role: 'user', content: 'How can I become a better developer?' }
embedding: [
   -0.033478674,         0.016010953,  -0.025884598,
    0.021905465,         0.014864422,    0.01289509,
    0.011276458,         0.004181462, -0.0009307125,
  0.00026049835,          0.02156825,  -0.036796864,
    // ... more dimensions
  [length]: 3072
]
```

This vector representation allows machines and LLMs to efficiently process and compare semantic meanings. Let's create these embeddings:

```python
from langchain_openai import OpenAIEmbeddings

# Initialize the embeddings model
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-large"    # Latest model for best quality
)

# Create a sample embedding to verify
sample_text = split_docs[0].page_content
sample_embedding = embeddings.embed_query(sample_text)
print(f"Embedding dimension: {len(sample_embedding)}")
```

We're using OpenAI's latest `text-embedding-3-large` model for optimal quality. The embeddings will have 3,072 dimensions, which allows for highly nuanced semantic representation of your content.

4. **Storing in Pinecone**: This cell creates your vector database:
```python
from langchain_pinecone import PineconeVectorStore

# Create your vector store
index_name = "your-index-name"    # Name this whatever you like
vectorstore = PineconeVectorStore.from_documents(
    split_docs, 
    embeddings, 
    index_name=index_name
)
```

When you store vectors in Pinecone, you can also attach metadata to each vector. This metadata is crucial for building rich experiences:

```python
# Example metadata structure
{
  "text": "In this article I reflect back on the year...",
  "source": "src/app/blog/2023-wins/page.mdx"
}
```

This metadata allows you to store additional information alongside your vectors, which you can use to enrich your application's responses or provide citations.

#### Testing Your Knowledge Store

The notebook includes cells for end-to-end testing. Let's verify everything works:

1. **Test Query Embedding**:
```python
# Convert a test query to embeddings
query = "What is RAG?"
query_embedding = embeddings.embed_query(query)
print(f"Query embedding dimension: {len(query_embedding)}")
```

2. **Test Vector Search**:
```python
# Search for similar content
results = vectorstore.similarity_search(
    query,
    k=3    # Number of results to return
)

# Display results
for i, doc in enumerate(results, 1):
    print(f"\nResult {i}:")
    print(f"Score: {doc.metadata.get('score', 'N/A')}")
    print(f"Source: {doc.metadata.get('source', 'N/A')}")
    print(f"Content: {doc.page_content[:200]}...")
```

This will show you the most relevant chunks of content for your query, along with their similarity scores and sources.

3. **Verify Metadata**:
```python
# Check what metadata is stored
sample_result = results[0]
print("Available metadata fields:")
for key, value in sample_result.metadata.items():
    print(f"{key}: {value}")
```

Your knowledge store is ready when:
- Queries return relevant results
- Metadata includes source information
- Similarity scores are reasonably high (> 0.7)

Now that your vector database is set up, we can move on to building the application that will use it.

### Step 2: Application Setup

Now that our vector database is ready, let's build the application:

1. Create a new Next.js project:
```bash
npx create-next-app@latest my-rag-app --typescript --tailwind --app
cd my-rag-app
```

2. Install dependencies:
```bash
npm install @pinecone-database/pinecone @vercel/ai ai openai-edge
```

3. Configure environment variables in `.env.local`:
```bash
OPENAI_API_KEY=your-key-here
PINECONE_API_KEY=your-key-here
PINECONE_INDEX=your-index-name
```

### Step 3: Backend Implementation

Create your API route at `src/app/api/chat/route.ts`:

```typescript
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { PineconeRecord } from "@pinecone-database/pinecone";
import { Metadata, getContext } from '../../services/context';
import { importArticleMetadata } from '@/lib/articles';
import path from 'path';
import { ArticleWithSlug } from '@/lib/shared-types';

export const maxDuration = 300;

export async function POST(req: Request) {
  const { messages } = await req.json();
  const lastMessage = messages[messages.length - 1];
  
  // Get context from Pinecone
  const context = await getContext(lastMessage.content, '', 3000, 0.8, false);
  
  // Process matches and build response
  let blogUrls = new Set<string>();
  let docs: string[] = [];
  
  (context as PineconeRecord[]).forEach(match => {
    const source = (match.metadata as Metadata).source;
    if (!source.includes('src/app/blog')) return;
    blogUrls.add((match.metadata as Metadata).source);
    docs.push((match.metadata as Metadata).text);
  });

  // Build related posts list
  let relatedBlogPosts: ArticleWithSlug[] = [];
  for (const blogUrl of blogUrls) {
    const blogPath = path.basename(blogUrl.replace('page.mdx', ''));
    const localBlogPath = `${blogPath}/page.mdx`;
    const { slug, ...metadata } = await importArticleMetadata(localBlogPath);
    relatedBlogPosts.push({ slug, ...metadata });
  }

  // Create context for LLM
  const contextText = docs.join("\n").substring(0, 3000);
  const prompt = `
    START CONTEXT BLOCK
    ${contextText}
    END OF CONTEXT BLOCK
    
    You are a helpful AI assistant. Use the context provided between the START CONTEXT BLOCK and END OF CONTEXT BLOCK tags to answer the user's question.
    If the context doesn't contain the answer, say "I don't have enough information to answer that question."
    Always cite your sources when possible.
  `;

  // Generate streaming response
  const result = streamText({
    model: openai.chat('gpt-4o'),
    system: prompt,
    prompt: lastMessage.content,
  });

  // Include related posts in response
  const serializedArticles = Buffer.from(
    JSON.stringify(relatedBlogPosts)
  ).toString('base64');

  return result.toDataStreamResponse({
    headers: {
      'x-sources': serializedArticles
    }
  });
}
```

### Step 4: Testing the Backend

Before building the frontend, let's test our backend:

1. Test vector search:
```typescript
const testSearch = await getContext("What is RAG?", '', 3000, 0.8, false);
console.log(testSearch);
```

2. Test the API endpoint:
```bash
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"What is RAG?"}]}'
```

### Step 5: Frontend Implementation

Create your chat interface at `src/app/chat/page.tsx`:

```typescript
'use client';

import { useChat } from 'ai/react';
import { useState } from 'react';
import { ArticleWithSlug } from '@/lib/shared-types';

export default function ChatPage() {
  const [articles, setArticles] = useState<ArticleWithSlug[]>([]);

  const { messages, input, handleInputChange, handleSubmit } = useChat({
    onResponse(response) {
      const sourcesHeader = response.headers.get('x-sources');
      const parsedArticles = sourcesHeader
        ? JSON.parse(atob(sourcesHeader)) as ArticleWithSlug[]
        : [];
      setArticles(parsedArticles);
    }
  });

  return (
    <div className="max-w-4xl mx-auto p-4">
      <div className="flex flex-col space-y-4">
        {messages.map(m => (
          <div key={m.id} className={m.role === 'user' ? 'text-blue-600' : 'text-green-600'}>
            <strong>{m.role === 'user' ? 'You:' : 'AI:'}</strong> {m.content}
          </div>
        ))}
        
        {articles.length > 0 && (
          <div className="mt-4">
            <h3 className="text-lg font-semibold">Related Articles:</h3>
            <ul className="list-disc pl-5">
              {articles.map(article => (
                <li key={article.slug}>
                  <a href={`/blog/${article.slug}`} className="text-blue-500 hover:underline">
                    {article.title}
                  </a>
                </li>
              ))}
            </ul>
          </div>
        )}
      </div>

      <form onSubmit={handleSubmit} className="mt-4">
        <input
          value={input}
          onChange={handleInputChange}
          placeholder="Ask a question..."
          className="w-full p-2 border rounded"
        />
      </form>
    </div>
  );
}
```

### Step 6: Performance Optimization

1. Monitor embedding costs:
```typescript
// Add cost tracking
let tokenCount = 0;
const COST_PER_1K_TOKENS = 0.0001;

// In your embedding function
tokenCount += estimateTokens(input);
console.log(`Estimated cost: $${(tokenCount/1000) * COST_PER_1K_TOKENS}`);
```

2. Implement rate limiting:
```typescript
// Add retry logic for API calls
const result = await retry(
  async () => await openai.createEmbedding(...),
  {
    retries: 3,
    minTimeout: 1000,
  }
);
```

3. Handle large datasets:
```typescript
// Stream large datasets in chunks
const chunkSize = 100;
for (let i = 0; i < docs.length; i += chunkSize) {
  const chunk = docs.slice(i, i + chunkSize);
  await processChunk(chunk);
}
```

### Step 7: Deployment

1. Create a new Vercel project:
```bash
vercel
```

2. Configure environment variables in Vercel:
- `OPENAI_API_KEY`
- `PINECONE_API_KEY`
- `PINECONE_INDEX`

3. Deploy:
```bash
vercel deploy --prod
```

### Next Steps

- Add authentication to protect your API routes
- Implement caching for frequently asked questions
- Add error boundaries and loading states
- Monitor and optimize your API usage

That's it! You now have a production-ready RAG pipeline. For support or questions, feel free to reach out in the comments below. 