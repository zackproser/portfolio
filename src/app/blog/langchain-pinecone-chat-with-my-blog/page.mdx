import { ArticleLayout } from '@/components/ArticleLayout'
import { Button } from '@/components/Button'
import Image from 'next/image'

import chatWithMyBlog  from '@/images/custom-rag-chat-screenshot.webp'
import chatWithMyBlogRelatedPosts from '@/images/custom-rag-chat-screenshot-dark.webp'
import chatWithBlogFlowchart from '@/images/chat-with-blog-flowchart.webp'
import ragWithSources from '@/images/rag-with-sources.webp'
import pineconeConsole from '@/images/pinecone-console-rag-pipeline.webp'

import ConsultingCTA from '@/components/ConsultingCTA'

import { createMetadata } from '@/utils/createMetadata'

export const articleMetadata = {
    author: "Zachary Proser", 
    date: "2024-05-10",
    title: "Build a RAG pipeline for your blog with Vercel AI SDK, OpenAI and Pinecone",
    description: "Learn how to build an AI chatbot that can intelligently answer questions about your blog content",
    image: chatWithMyBlog,
    slug: 'langchain-pinecone-chat-with-my-blog',
    isPaid: true,
    price: 2000,
    previewLength: 250,
    previewElements: 37,
    paywallHeader: "Master RAG Pipelines: The Most In-Demand Gen AI Skill",
    paywallBody: "Every company wants developers who can build RAG applications. Get a complete, production-ready tutorial that teaches you exactly how to build a RAG pipeline with the latest tech stack: Vercel AI SDK, OpenAI embeddings, and Pinecone vector search.",
    buttonText: "Level Up Your Gen AI Skills ($20)"
};

export const metadata = createMetadata(articleMetadata);

export default (props) => <ArticleLayout metadata={articleMetadata} {...props} />

I built [a chat with my blog experience](/chat) into my site, allowing visitors to ask questions of my writing using modern AI tools and techniques.

Here's a quick demo of it in action - or [you can try it out yourself](/chat): 

<iframe 
  className="youtube-video"
  src="https://www.youtube.com/embed/ii4aUE-6Okk" 
  title="Building an AI chatbot with Vercel AI SDK, Pinecone.io and OpenAI" 
  frameBorder="0" 
  allow="fullscreen;">
 </iframe>

<Image src={chatWithMyBlog} alt="I built a chat with my blog feature" />

My solution recommends related blog posts that were used to answer the question as the LLM responds. This is [Retrieval Augmented Generation](https://pinecone.io/learn/retrieval-augmented-generation) 
with citations. 

In this tutorial, you'll learn how to build your own similar experience using:

* The Vercel AI SDK for efficient streaming responses
* OpenAI's latest models for embeddings and chat
* Pinecone for vector similarity search
* Next.js 14 for the modern web application framework

<Image src={chatWithMyBlogRelatedPosts} alt="Related blog posts" />

Best of all, this site is [completely open-source](https://github.com/zackproser/portfolio), so you can view and borrow my implementation. 

## Table of contents

## Architecture and data flow

Here's a flowchart describing how the feature works end to end. 

<Image src={chatWithBlogFlowchart} alt="Chat with my blog flowchart" />

Let's talk through it from the user's perspective. They ask a question on my client-side chat interface. Their question is sent to my `/api/chat` route. 

The chat route first converts the user's natural language query to embeddings using OpenAI's latest `text-embedding-3-large` model, and then performs a vector search against Pinecone. 

Embeddings or vectors are lists of floating point numbers that represent semantic meaning and the relationships between entities in data in a format that is efficient for machines and Large Language Models to parse.

For example, in the following log statements, you can see the user's query, 'How can I become a better developer?', and the query embedding which is an array of floats representing that sentence. 

```bash 
lastMessage: { role: 'user', content: 'How can I become a better developer?' }
embedding: [
   -0.033478674,         0.016010953,  -0.025884598,
    0.021905465,         0.014864422,    0.01289509,
    // ... many more dimensions
  [length]: 3072
]
```

Note the length of this array is `3072`. This is the dimensionality expressed by the embedding model, which is `text-embedding-3-large`. I chose this over the cheaper
`text-embedding-3-small` model because my primary concern is accuracy.

## Server side implementation

Let's look at how the chat API route works with the latest Vercel AI SDK and OpenAI client. Here's the complete implementation:

```typescript
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { PineconeRecord } from "@pinecone-database/pinecone"
import { Metadata, getContext } from '../../services/context'
import { importArticleMetadata } from '@/lib/articles'
import { ArticleWithSlug } from '@/lib/shared-types';

export async function POST(req: Request) {
  const { messages } = await req.json();
  const lastMessage = messages[messages.length - 1]

  // Get relevant context from Pinecone using vector similarity search
  const context = await getContext(lastMessage.content, '', 3000, 0.35, false)

  // Track blog URLs for related posts
  let blogUrls = new Set<string>()
  let docs: string[] = [];

  (context as PineconeRecord[]).forEach(match => {
    const source = (match.metadata as Metadata).source
    if (!source.includes('src/app/blog')) return
    blogUrls.add((match.metadata as Metadata).source);
    docs.push((match.metadata as Metadata).text);
  });

  // Get metadata for related blog posts
  let relatedBlogPosts: ArticleWithSlug[] = []
  for (const blogUrl of blogUrls) {
    const blogPath = path.basename(blogUrl.replace('page.mdx', ''))
    const localBlogPath = `${blogPath}/page.mdx`
    const { slug, ...metadata } = await importArticleMetadata(localBlogPath);
    relatedBlogPosts.push({ slug, ...metadata });
  }

  // Join context text with a reasonable token limit
  const contextText = docs.join("\n").substring(0, 3000)

  // Create system prompt with context
  const prompt = `
    Zachary Proser is a Staff developer, open-source maintainer and technical writer.
    Zachary will take into account the following context to answer questions:
    START CONTEXT BLOCK
    ${contextText}
    END OF CONTEXT BLOCK
    If the context doesn't provide the answer, Zachary will say "I don't know the answer to that question".
    Zachary will not invent anything not drawn from the context.
  `;

  // Use Vercel AI SDK to stream the response
  const result = streamText({
    model: openai.chat('gpt-4'),
    system: prompt,
    prompt: lastMessage.content,
  });

  // Attach related articles as base64 encoded header
  const serializedArticles = Buffer.from(
    JSON.stringify(relatedBlogPosts)
  ).toString('base64')

  return result.toDataStreamResponse({
    headers: {
      'x-sources': serializedArticles
    }
  });
}
```

The key improvements in this implementation include:

1. Using `streamText` from Vercel AI SDK for efficient streaming responses
2. Modern OpenAI client configuration with `@ai-sdk/openai`
3. Improved context handling with better token management
4. Efficient metadata handling for related posts

## Client-side implementation

The client-side chat interface is built using React and the Vercel AI SDK's `useChat` hook. Here's the implementation:

```typescript
'use client'

import { Container } from '@/components/Container';
import { useState } from 'react';
import { useChat } from 'ai/react';
import { track } from '@vercel/analytics';
import { clsx } from 'clsx';
import { ArticleWithSlug } from '@/lib/shared-types';
import { BlogPostCard } from '@/components/BlogPostCard';

const prepopulatedQuestions = [
  "What is the programming bug?",
  "Why do you love Next.js so much?",
  "What do you do at Pinecone?",
  // ... more questions
];

export default function ChatPageClient() {
  const [isLoading, setIsLoading] = useState(false);
  const [articles, setArticles] = useState<ArticleWithSlug[]>([]);

  const { messages, input, setInput, handleSubmit } = useChat({
    onResponse(response) {
      // Parse related articles from response headers
      const sourcesHeader = response.headers.get('x-sources');
      const parsedArticles: ArticleWithSlug[] = sourcesHeader
        ? (JSON.parse(atob(sourcesHeader as string)) as ArticleWithSlug[])
        : [];
      setArticles(parsedArticles);
      setIsLoading(false);
    },
    onFinish() {
      // Track chat analytics
      track("chat", { question: input });
    },
    onError() {
      setIsLoading(false);
    }
  });

  const handleSearch = async (query: string) => {
    setInput(query);
    track("chat-precanned", { question: query });
    
    const customSubmitEvent = {
      preventDefault: () => { },
    } as unknown as React.FormEvent<HTMLFormElement>;

    await handleSubmit(customSubmitEvent);
  };

  return (
    <Container>
      <div className="max-w-7xl mx-auto mt-16 sm:mt-32">
        {/* Chat interface */}
        <div className="mb-8">
          <SearchForm
            suggestedSearches={prepopulatedQuestions}
            onSearch={handleSearch}
            setIsLoading={setIsLoading}
          />
        </div>

        {isLoading && messages?.length > 0 && <LoadingAnimation />}

        {/* Chat messages and related posts */}
        <div className="flex flex-col md:flex-row">
          <div className="flex-1 pr-0 md:pr-6 mb-6 md:mb-0">
            {messages.map((m) => (
              <div
                key={m.id}
                className="mb-4 whitespace-pre-wrap text-lg leading-relaxed"
              >
                <span
                  className={clsx('font-bold', {
                    'text-blue-700': m.role === 'user',
                    'text-green-700': m.role !== 'user',
                  })}
                >
                  {m.role === 'user' ? 'You: ' : "AI: "}
                </span>
                {m.content}
              </div>
            ))}
          </div>
          
          {/* Related posts sidebar */}
          <div className="md:w-1/3">
            {Array.isArray(articles) && (articles.length > 0) && (
              <div>
                <h3 className="mb-4 text-xl font-semibold">Related Posts</h3>
                <div className="space-y-4">
                  {articles.map((article) => (
                    <BlogPostCard key={article.slug} article={article} />
                  ))}
                </div>
              </div>
            )}
          </div>
        </div>
      </div>
    </Container>
  );
}
```

The key improvements in the client implementation include:

1. Modern React hooks and state management
2. Efficient message streaming with Vercel AI SDK
3. Improved error handling and loading states
4. Analytics integration with Vercel Analytics
5. Responsive design with Tailwind CSS

## Project Setup

To get started, you'll need to set up a Next.js project with the required dependencies. Here's how:

1. Create a new Next.js project:

```bash
npx create-next-app@latest my-chat-blog --typescript --tailwind --app
cd my-chat-blog
```

2. Install the required dependencies:

```bash
npm install ai @vercel/analytics @pinecone-database/pinecone clsx
```

3. Set up your environment variables in `.env.local`:

```bash
OPENAI_API_KEY=your_openai_api_key
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=your_pinecone_environment
PINECONE_INDEX=your_index_name
```

4. Create the necessary directories:

```bash
mkdir -p src/app/api/chat
mkdir -p src/app/chat
mkdir -p src/services
```

This setup provides the foundation for building your chat interface. The complete implementation includes:

- Vector embeddings generation for your blog content
- Pinecone index setup and management
- API route for handling chat requests
- Client-side chat interface
- Related posts recommendations

## What's included in the full tutorial?

When you purchase the full tutorial for $15, you'll get access to:

1. **Complete Vector Pipeline Implementation**
   - Step-by-step guide to process your blog content
   - Code for generating embeddings efficiently
   - Pinecone index management and optimization

2. **Advanced Server-Side Features**
   - Error handling and rate limiting
   - Token optimization for cost efficiency
   - Caching strategies for better performance

3. **Enhanced Client Features**
   - Advanced UI components with animations
   - Error boundary implementation
   - Loading state management
   - Analytics integration

4. **Deployment Guide**
   - Step-by-step Vercel deployment
   - Environment configuration
   - Performance monitoring setup

5. **Bonus Content**
   - Tips for prompt engineering
   - Cost optimization strategies
   - Scaling considerations
   - Security best practices

The complete tutorial includes all source code, explanations, and best practices learned from building and maintaining this feature in production.
