import { ArticleLayout } from '@/components/ArticleLayout'
import { Button } from '@/components/Button'
import Image from 'next/image'

import embeddingsModelTrainedOnRag from '@/images/embeddings-trained-rag.webp'
import ragPipelineQuery from '@/images/rag-query-pipeline.webp'
import embeddingRagTraining from '@/images/embedding-rag-training.webp'
import truncateModelLayers from '@/images/truncate-model-layers.webp'
import embeddingsRagFlowchart from '@/images/embeddings-rag-training-flowchart.webp'

import ConsultingCTA from '@/components/ConsultingCTA'

import { createMetadata } from '@/utils/createMetadata'

export const metadata = createMetadata({
    author: "Zachary Proser", 
    date: "2024-06-05",
    title: "How are embeddings models trained for RAG?",
    description: "Embeddings models are the secret sauce that makes RAG work. How are THEY made?",
    image: embeddingsModelTrainedOnRag
});

export default (props) => <ArticleLayout metadata={metadata} {...props} />

--- 

You can ask your RAG pipeline, "What line is the bug on?", and it will tell you the answer almost instantly. How? 

<Image src={ragPipelineQuery} alt="Asking your RAG pipeline a question" />

Embeddings models are the secret sauce that makes RAG work so well. How are they trained in this "asking questions of documents" use case?

In this blog post we'll unpack how embeddings models like OpenAI's `text-embedding-3-large` are trained to support this document retrieval and chat use case. 

## Table of contents

## Training Data and Labels
In the context of training an embedding model for RAG, the training data usually consists of pairs of queries and documents. 

The labels aren't traditional categorical labels; instead, they are used to create pairs of similar (positive) and dissimilar (negative) document embeddings.

* **Positive pairs**: Queries and their relevant documents.
* **Negative pairs**: Queries and irrelevant documents (often sampled randomly or using hard negatives).

Here's what a simple training example might look like in Python: 

```python
import random

# Sample data
queries = ["What is RAG?", "Explain embeddings.", "How to train a model?"]
documents = [
    "RAG stands for Retrieval-Augmented Generation.",
    "Embeddings are vector representations of text.",
    "Model training involves adjusting weights based on loss."
]

# Function to create positive and negative pairs
def create_pairs(queries, documents):
    pairs = []
    labels = []
    
    for query in queries:
        # Positive pair (query and relevant document)
        positive_doc = random.choice(documents)
        pairs.append((query, positive_doc))
        labels.append(1)
        
        # Negative pair (query and irrelevant document)
        negative_doc = random.choice([doc for doc in documents if doc != positive_doc])
        pairs.append((query, negative_doc))
        labels.append(0)
    
    return pairs, labels

pairs, labels = create_pairs(queries, documents)
for pair, label in zip(pairs, labels):
    print(f"Query: {pair[0]} \nDocument: {pair[1]} \nLabel: {label}\n")
 
````

## Model Architecture
Many modern embedding models are based on transformer architectures, such as BERT, RoBERTa, or specialized models like Sentence-BERT (SBERT). These models typically output token-level embeddings.

* **Token-level embeddings**: Each token (word or subword) in the input sequence gets its own embedding vector. I built [a demo showing what word and subword tokens look like here](/demos/tokenize).
* **Pooling mechanism**: To get a single vector representation of the *entire document or query*, a pooling mechanism (like mean pooling or taking the [CLS] token embedding) is applied to the token-level embeddings.

## Loss Functions
The training objective is to learn embeddings such that queries are close to their relevant documents in the vector space and far from irrelevant documents. 

Common loss functions include:

* **Contrastive loss**: Measures the distance between positive pairs and minimizes it, while maximizing the distance between negative pairs. See also Geoffrey Hinton's paper on [Contrastive Divergence](http://www.cs.toronto.edu/~hinton/absps/nccd.pdf).
* **Triplet loss**: Involves a triplet of (query, positive document, negative document) and aims to ensure that the query is closer to the positive document than to the negative document by a certain margin. This [paper on FaceNet](https://arxiv.org/abs/1503.03832) describes using triplets, and [this repository](https://github.com/davidsandberg/facenet) has code samples.
* **Cosine similarity loss**: Maximizes the cosine similarity between the embeddings of positive pairs and minimizes it for negative pairs.

## Training Procedure

<Image src={embeddingRagTraining} alt="Training an embedding model for RAG" />

The training process involves feeding pairs of queries and documents through the model, obtaining their embeddings, and then computing the loss based on the similarity or dissimilarity of these embeddings.

<Image src={embeddingsRagFlowchart} alt="Training an embedding model for RAG" />

* **Input pairs**: Query and document pairs are fed into the model.
* **Embedding generation**: The model generates embeddings for the query and document.
* **Loss computation**: The embeddings are used to compute the loss (e.g., contrastive loss, triplet loss).
* **Backpropagation**: The loss is backpropagated to update the model weights.

## Embedding Extraction
<Image src={truncateModelLayers} alt="Truncating model layers" />
After training, the model is often truncated to use only the layers up to the point where the desired embeddings are produced. 

For instance:

* **Final layer embeddings**: In many cases, the embeddings from the final layer of the model are used.
* **Intermediate layer embeddings**: Sometimes, embeddings from an intermediate layer are used if they are found to be more useful for the specific task.

## Let's consider a real example

Sentence-BERT (SBERT) is a good example of a model specifically designed for producing sentence-level embeddings.

* **Model architecture**: Based on BERT, but with a pooling layer added to produce a fixed-size vector for each input sentence.
* **Training data**: Uses pairs of sentences with a label indicating if they are similar or not.
* **Training objective**: Uses a Siamese network structure and contrastive loss to ensure that similar sentences have embeddings close to each other and dissimilar sentences have embeddings far apart.

## Summary

<Image src={embeddingsModelTrainedOnRag} alt="Embeddings model trained on RAG" />

Training an embedding model for Retrieval Augmented Generation use cases requires a few key components:

* **Training data**: Pairs of queries and documents (positive and negative).
* **Model output**: Typically token-level embeddings pooled to create sentence/document-level embeddings.
* **Loss functions**: Contrastive loss, triplet loss, cosine similarity loss.
* **Training**: Involves generating embeddings, computing loss, and updating model weights.
* **Embedding extraction**: Uses final or intermediate layer embeddings after training.

