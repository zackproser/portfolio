import { ArticleLayout } from '@/components/ArticleLayout'
import { Button } from '@/components/Button'
import Image from 'next/image'

import mlOps from '@/images/mlops.webp'
import zpMLOps from '@/images/zp-mlops.webp'
import zpLlama from '@/images/zp-llama.webp'
import myWriting from '@/images/my-writing.webp'
import civitai from '@/images/civitai.webp'
import pumpkinPatch from '@/images/pumpkin-patch.webp'
import halloweenLora from '@/images/halloween-lora.webp'
import pumpkinLora from '@/images/pumpkin-lora.webp'
import neuralNetworkFineTuning from '@/images/neural-network-fine-tuning.webp'
import neuralNetworkLoraAdaper from '@/images/neural-network-lora-adapter.webp'
import googleColab from '@/images/google-colab.webp'

import ConsultingCTA from '@/components/ConsultingCTA'

import { createMetadata } from '@/utils/createMetadata'

export const metadata = createMetadata({
    author: "Zachary Proser",
    date: "2024-09-20",
    title: "Teaching Llama 3.1 to Write Like Me: An LLM Fine-tuning Adventure with Google Colab, Torchtune, and Weights & Biases",
    description: "I wanted hands-on experience with fine-tuning LLMs, so I used all my writing as training data.",
    image: mlOps,
    slug: '/blog/how-to-fine-tune-llama-3-1-with-torchtune'
});

export default (props) => <ArticleLayout metadata={metadata} {...props} />

<Image src={mlOps} alt="Getting hands on with MLOps" />
<figcaption>Getting hands on with MLOps as an application and infrastructure developer</figcaption>

## Table of contents

## Why I Wanted to Fine-Tune Llama 3.1
Neural networks are too fascinating to ignore. As an application and infrastructure developer by background, I'm building side projects to get hands-on with neural networks, MLOps, and the intricacies of training models and building inference endpoints. 

Recently, [I trained a neural net to recognize hand-drawn digits, wrapped it in an inference endpoint, and turned it into a web app.](/blog/mnist-pytorch-hand-drawn-digit-recognizer)

<Image src={zpLlama} alt="Llama 3.1" />
<figcaption>I want to fine-tune Llama 3.1 on my writing.</figcaption>

Now, I want to scratch a fine-tuning itch. Can I show Meta's Llama 3.1-8B-Instruct model enough of my writing that it can write passages or articles in my style? 

I wanted to try Torchtune, a native PyTorch library that simplifies finetuning, evaluation, generation tasks, Weights and Biases, Google Colab Pro, and some beefy GPUs. I looked for tedium, frustration, and sharp edges and was not disappointed.

This post is the first in a series exploring the tools and techniques available for fine-tuning LLMs, covering data preparation, model selection and how to fine-tune LLMs using Torchtune. Future posts will delve deeper into improving our results, evaluating the model's performance, and creating user-friendly interfaces for text generation.

The companion GitHub repository for this post is here. It contains the Jupyter Notebooks I used for this project and a utility script for cleaning my writing corpus.

## The Project: Teaching Llama 3.1 to Write Like Me

<Image src={zpMLOps} alt="Getting hands on with MLOps" />
<figcaption>Getting hands on with MLOps so that I can learn the patterns, tooling and infrastructure required for productionizing Large Language Models.</figcaption>

### Model Selection: Llama 3.1-8B-Instruct

For this project, I chose Meta's Llama 3 as the base model. Specifically, I used the Llama 3.1-8B-instruct version. 

Instruction-trained models like Llama 3.1 are pre-trained on a massive corpus of diverse data, including books, web pages, and other resources. This makes them highly versatile and capable of generating a wide range of outputs, based on user instructions:

> **Instruction**: "Write a short story about a robot learning to cook."
>
> **Response**: "In a bustling kitchen of the future, Robot X-5 stood motionless, its optical sensors fixed on the sizzling pan before it..."
> 

### Data Preparation

You can find the companion Jupyter Notebook for this section here.

One of the most critical aspects of fine-tuning an LLM is preparing your dataset in the correct format. It took some uncovering and digging, but I found that the [Alpaca dataset](https://github.com/tatsu-lab/alpaca_dataset) is the correct format for fine-tuning Llama 3.1 models.

#### Why the Alpaca Format Matters

The Alpaca dataset format is a specific structure used for instruction-tuning language models. It consists of three key components:

1. `instruction`: A prompt or question that guides the model's response.
2. `input`: Additional context or information (can be empty).
3. `output`: The desired response from the model.

```json
[
  {
    "instruction": "Write a short story about a robot learning to cook.",
    "input": "",
    "output": "In a bustling kitchen of the future, Robot X-5 stood motionless, its optical sensors fixed on the sizzling pan before it. 'Crack two eggs,' the recipe instructed. With precision, X-5 selected an egg from the carton, but its metallic fingers crushed the delicate shell, sending yolk dripping down its chassis. Undeterred, X-5 tried again, this time adjusting its grip strength. Success! The egg cracked perfectly into the pan. As the day progressed, X-5 learned to chop vegetables without pulverizing them and to measure ingredients with increasing accuracy. By evening, a slightly lopsided but edible omelet sat on a plate, steam rising from its golden surface. X-5's LED display flickered in what could only be described as pride. It had taken its first step in mastering the art of cooking, proving that even robots can find joy in creating something delicious."
  },
  {
    "instruction": "Explain the concept of photosynthesis in simple terms.",
    "input": "",
    "output": "Photosynthesis is like a plant's way of making its own food. Imagine plants as tiny factories that use sunlight as their power source. They take in water from the soil through their roots and carbon dioxide from the air through tiny holes in their leaves. Then, using the energy from sunlight, they mix these ingredients together to make glucose, which is a type of sugar that plants use for food. As a bonus, this process also produces oxygen, which is released into the air for us and other animals to breathe. So, in simple terms, photosynthesis is how plants turn sunlight, water, and air into food and oxygen."
  },
  {
    "instruction": "Describe the steps to change a flat tire.",
    "input": "",
    "output": "1. Ensure your car is safely parked on a flat surface, away from traffic. Turn on your hazard lights.\n2. Locate your spare tire, jack, and lug wrench in your vehicle.\n3. Loosen the lug nuts on the flat tire slightly, but don't remove them completely yet.\n4. Place the jack under the designated lifting point of your car and raise the vehicle until the flat tire is off the ground.\n5. Remove the lug nuts completely and take off the flat tire.\n6. Place the spare tire onto the wheel hub.\n7. Replace the lug nuts and tighten them by hand.\n8. Lower the car back to the ground using the jack.\n9. Use the lug wrench to fully tighten the lug nuts in a star pattern for even pressure.\n10. Store the flat tire, jack, and lug wrench back in your vehicle.\n11. Check the spare tire's pressure and drive carefully, following any speed restrictions for your specific type of spare tire."
  }
]
```

Getting the dataset format right is crucial for several reasons:

1. **Compatibility**: Many finetuning scripts and libraries, including torchtune, expect data in this specific format.
2. **Error Prevention**: An incorrect format can cause your finetuning run to fail midway, wasting valuable time and computational resources.
3. **Model Performance**: The format helps the model understand the structure of prompts and responses, leading to better finetuning results.

#### Preparing My Own Writing as Training Data

[My website](https://zackproser.com) is fully open-source and it's built with Next.js, and MDX for content. MDX is Markdown interspersed with JSX / JavaScript components.

<Image src={myWriting} alt="My writing" />

Getting my whole site within a Jupyter Notebook is a single command: 

```bash 
!git clone https://github.com/zackproser/portfolio.git
```

Next, I needed to extract the content from the MDX files. To begin with, I'm going to use all the articles under `src/app/blog`.

#### Data Processing Pipeline

To prepare my data, I implemented a series of functions to process my MDX files:

```python
def markdown_to_text(markdown_string):
    """Converts a markdown string to plain text."""
    html = markdown(markdown_string)
    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text()
    return text

def clean_text(text):
    """Clean text by removing extra whitespace and normalizing."""
    # Remove empty lines and excess whitespace
    cleaned_lines = [line.strip() for line in text.splitlines() if line.strip()]

    # Break down long lines into sentences
    cleaned_text = []
    for line in cleaned_lines:
        if len(line) > 200:  # Example threshold for long lines
            sentences = re.split(r'(?<=[.!?]) +', line)
            cleaned_text.extend(sentences)
        else:
            cleaned_text.append(line)

    return "\n".join(cleaned_text)

def process_mdx_files(directory):
    """Process all MDX files in the given directory and its subdirectories."""
    processed_content = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.endswith('.mdx'):
                file_path = os.path.join(root, file)
                with open(file_path, 'r', encoding='utf-8') as f:
                    raw_text = f.read()
                plain_text = markdown_to_text(raw_text)
                cleaned_text = clean_text(plain_text)
                processed_content.append(cleaned_text)
    return processed_content

def extract_metadata(content):
    """Extract metadata from the content."""
    metadata = {}
    metadata_match = re.search(r'export const metadata = createMetadata\((.*?)\)', content, re.DOTALL)
    if metadata_match:
        metadata_str = metadata_match.group(1)
        # Extract key-value pairs
        for match in re.finditer(r'(\w+):\s*["\']?(.*?)["\']?,?\s*(?=\w+:|$)', metadata_str):
            key, value = match.groups()
            metadata[key.strip()] = value.strip().strip('"').strip("'")
    return metadata

def clean_content(content):
    """Clean the content by removing imports, exports, metadata, and special characters."""
    # Remove import statements
    content = re.sub(r'import.*\n', '', content)
    # Remove export statements and metadata
    content = re.sub(r'export const metadata.*?}\)', '', content, flags=re.DOTALL)
    content = re.sub(r'export default.*\n', '', content)
    # Remove special characters and markdown syntax
    content = re.sub(r'[#*`]', '', content)
    # Remove empty lines and lines that look like object properties
    content = '\n'.join(line for line in content.split('\n') if line.strip() and not re.match(r'^\w+:\s', line))
    return content.strip()
```

These functions handle the conversion of MDX to plain text, clean the text, and process all MDX files in a directory.

Here's how I implemented the Alpaca format for my project:

```python
def create_alpaca_entry(content):
    """Create an Alpaca format entry for the given content."""
    metadata = extract_metadata(content)
    cleaned_content = clean_content(content)

    article_name = metadata.get('title', 'an article').strip('"')

    return {
        "instruction": f'Write an article about "{article_name}"',
        "input": "",
        "output": cleaned_content
    }
```

This function takes the content of an MDX file, extracts the metadata (including the title), cleans the content, and formats it into the Alpaca structure.

#### Creating the Final Dataset

After processing the MDX files, I created the final Alpaca-formatted dataset:

```python
alpaca_data = [create_alpaca_entry(content) for content in mdx_content]

# Save Alpaca format data to a file
with open('training_data.jsonl', 'w', encoding='utf-8') as f:
    json.dump(alpaca_data, f, ensure_ascii=False, indent=2)

print(f"Processed {len(alpaca_data)} entries and saved to training_data.jsonl")
```

This step creates an Alpaca-formatted entry for each processed MDX file and saves the entire dataset to a JSONL (JSON Lines) file.

I run this Data preparation Jupyter Notebook on Google Colab, so the final `training_data.jsonl` file is written to `/content/training_data.jsonl` by default, 
allowing me to download it.

In the end, my `training_data.jsonl` file is full of entries like this: 

>
> "**input**": "Write me an article about \"Terminal velocity - how to get faster as a developer\"", 
> "**output**": "\"You could be the greatest architect in the world, but that won't matter much if it takes you forever to type 
> everything into your computer.\" Hugo Posca\nWhy read this article?\nWhen you're finished reading this article, you'll understand 
> the why and how behind my custom development setup, which has subjectively made me much faster and happier in my day to day work.\n
> Here's a screenshot of my setup, captured from a streaming session..."
>

#### Lessons Learned

Preparing the dataset was one of the hardest parts of this project. Not because the work was complicated, but because it's difficult to discover 
which dataset format a given model is trained on. 

Here are some other takeaways:

1. **Understand Your Data**: Know the structure and peculiarities of your source data (in my case, MDX files with front matter).
2. **Clean Thoroughly**: Remove any artifacts that could confuse the model, like code snippets or markdown syntax.
3. **Validate Your Output**: Always check a sample of your processed data to ensure it's formatted correctly. You could burn a bunch of training time and money on stupid errors - or run your dataset through a JSONL validator first.
4. **Iterate**: Don't be afraid to refine your data processing pipeline as you learn more about what works and what doesn't.

By taking the time to get the data preparation right, you set a solid foundation for the rest of your finetuning process. 

### The Rich Do Not Finetune Like You And I - The Power of LoRA

Your choice in methods of fine-tuning Large Language Models is determined by your answer to the question, "Do you have access to hundreds of millions of dollars in capital?". 

If your answer is no, you're probably looking at single or multi-device LoRA or qLoRA - two methods of "steering" a foundational model toward different outputs without incurring the massive resource and time needs of full fine-tuning. 

<Image src={neuralNetworkFineTuning} alt="Neural Network Fine-Tuning" />
<figcaption>Neural Network Full Parameter Fine-Tuning may provide the best results, but it requires resources far beyond the grasp of your average solo developer or even medium-sized company.</figcaption>

The idea behind full fine-tuning is that you update all the weights of the model's parameters - and while it can potentially yield the best results, it requires resources far beyond the grasp of your average solo developer or even medium-sized company.

Sam Altman has publicly stated that OpenAI spent around $100M on cloud resources (RAM and warm GPUs in cloud servers) to train ChatGPT 4. 

The rest of us can look into PEFT or Parameter-efficient fine-tuning. For this project, I opted for LoRA, which stands for Low-Rank Adaptation. LoRA is a clever technique that allows you to "steer" a foundation model toward a specific domain or style. 

<Image src={neuralNetworkLoraAdaper} alt="Neural Network LoRA Adapter" />
<figcaption>LoRA is a clever technique that allows you to "steer" a foundation model toward a specific domain or style, without retraining the entire model or burning hundreds of millions of dollars in cloud resources.</figcaption>

To make this concept more concrete, consider the example of civit.ai, a website that allows users to create and share LoRA adapters and base models for image generation. 

<Image src={civitai} alt="Civit.ai" />

Imagine you're generating images of a pumpkin patch for a Halloween event. The base model gives you traditional fall-themed images: pumpkins, hot cider, and hay bales.

<Image src={pumpkinPatch} alt="Pumpkin Patch" />

But you look up a spooky Halloween-themed LoRA adapter...

<Image src={halloweenLora} alt="Halloween LoRA" />

By applying a Halloween-themed LoRA adapter alongside the base model's weights, you can add spooky elements to your generated images without retraining the entire model.

<Image src={pumpkinLora} alt="Pumpkin Patch" />

This example illustrates how LoRA can efficiently steer a model towards a specific style or domain, which is analogous to how I used it to guide Llama 3.1 towards my writing style.

###  Tools and Infrastructure

- **PyTorch and torchtune**: Torchtune is a native PyTorch library that provides helpful "recipes" (YAML configuration templates) for various steps in the model lifecycle.
- **Google Colab Pro**: For access to GPU resources necessary for efficient training. Specifically, I ran my finetuning tasks on an A100 GPU, which significantly accelerated the process.
- **Weights and Biases (W&B)**: For logging and visualizing metrics during training runs.
- **Hugging Face model hub**: For hosting my custom dataset and the finetuned model.

I chose these tools for their compatibility (torchtune being a native PyTorch library), accessibility (Colab Pro for easy and fast GPU access), and robust features for experiment tracking and model hosting.

## Fine-tuning Process: Leveraging Torchtune and Weights & Biases

### Why train on Google Colab?

<Image src={googleColab} alt="Google Colab" />
<figcaption>Even when paying for Google Colab Pro, it's pretty easy to max out your GPU or RAM usage when fine-tuning a Large Language Model.</figcaption>

I intentionally chose to fine-tune my model on Google Colab because I wanted to understand what is tedious and difficult about doing so. There are many different tools and platforms and an increasing number of third party APIs designed to simplify fine-tuning. I did not want to use these - not at first.

Llama 3.1 8B-instruct is a smaller Large Language Model that can do advanced language tasks with less computing power. This makes it easier for people with limited resources to customize the model for specific uses. 

However, fine-tuning this model on platforms like Google Colab can still be challenging, even with powerful hardware:

* **Limited GPU memory**: Even though the model is smaller, it still needs a lot of memory to fine-tune. Using larger batch sizes can quickly use up all available memory.
* **Time limits**: Colab sessions have time limits, usually around 12 hours for free users. Longer fine-tuning tasks might be interrupted before they finish.
* **Resource restrictions**: Heavy use of Colab can lead to less access to powerful GPUs or longer wait times.
* **Setup errors**: Mistakes in preparing data or setting up the fine-tuning process might only show up later, wasting time and resources. 

I easily nuked about $60 in mis-configured training runs before I got things right. This is exactly the kind of pain I went looking for. As a result, I learned to:

* Plan my fine-tuning tasks carefully
* Optimize my code for efficiency (choosing smaller batch sizes, quantizing weights, etc.)
* Monitor my resource use via visualization and experiment tracking tools such as Weights & Biases
* Be prepared for possible interruptions
* Double-check my setup to avoid late-stage failures

These steps help make the most of limited resources and increase the chances of successful fine-tuning.j

 We're ready to get fine-tuning! 

### 1. Setting Up the Environment

First, we need to install the necessary libraries:

```python
!pip install -U torchtune==0.2.1 torchao wandb peft sentencepiece transformers
```

Then, we log in to Weights & Biases for experiment tracking:

```python
import wandb
wandb.login()
```

### 2. Downloading the Base Model

We use the `tune download` command to download the Meta-Llama-3.1-8B-Instruct model:

```python
!tune download meta-llama/Meta-Llama-3.1-8B-Instruct --ignore-patterns=null
```

This command downloads the model and its weights, storing them in `/tmp/Meta-Llama-3.1-8B-Instruct` by default.

### 3. Configuring the Fine-tuning Process

One of the most critical steps is setting up the configuration for fine-tuning. I used a YAML file to define all the parameters. Here's a breakdown of some key sections:

```yaml
# Model Arguments
model:
  _component_: torchtune.models.llama3_1.qlora_llama3_1_8b
  lora_attn_modules: ['q_proj', 'v_proj', 'k_proj', 'output_proj']
  apply_lora_to_mlp: True
  apply_lora_to_output: False
  lora_rank: 8
  lora_alpha: 16

# Dataset and Sampler
dataset:
  _component_: torchtune.datasets.instruct_dataset
  source: json
  data_files: 'training_data.jsonl'
  template: torchtune.data.AlpacaInstructTemplate
  train_on_input: True
  split: train

# Training
epochs: 2
max_steps_per_epoch: null
gradient_accumulation_steps: 8
compile: False

# Logging
output_dir: /tmp/qlora_finetune_output/
metric_logger:
  _component_: torchtune.utils.metric_logging.WandBLogger
  project: write_like_me
```

This configuration uses QLoRA (Quantized Low-Rank Adaptation) for efficient fine-tuning, specifies the dataset format, and sets up Weights & Biases for logging.

### 4. Running the Fine-tuning Process

With the configuration in place, we can start the fine-tuning process:

```python
!tune run lora_finetune_single_device --config llama_wandb_qlora.yaml
```

This command kicks off the fine-tuning process using the configuration we defined. The process will log metrics to Weights & Biands, allowing us to monitor the training in real-time.

### 5. Verifying the Fine-tuned Model

After the fine-tuning process is complete, it's crucial to verify that our model is working as expected. We can do this by loading the model and generating some text:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from peft import PeftModel
import torch

model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct"
new_model = "zackproser/Meta-Llama-3.1-8B-instruct-zp-writing-ft-qlora"

base_model = AutoModelForCausalLM.from_pretrained(model_name)
model = PeftModel.from_pretrained(base_model, new_model)
model = model.merge_and_unload()

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=2000, device=0)

prompt = "Write me an article about getting faster as a developer"
result = pipe(f"<s>[INST] {prompt} [/INST]")
print(result)
```

This code loads our fine-tuned model, creates a text generation pipeline, and generates an article based on a prompt.

### 6. Publishing the Fine-tuned Model

Finally, we can upload our fine-tuned model to Hugging Face Hub for easy access and sharing:

```python
!huggingface-cli upload Meta-Llama-3.1-8B-instruct-zp-writing-ft-qlora /tmp/Meta-Llama-3.1-8B-Instruct-zp-finetuned
```

### Lessons Learned from Fine-tuning

1. **Configuration is Key**: Spending time to get the configuration right pays off. It determines everything from how your model learns to how efficiently it uses computational resources.

2. **Monitor Your Training**: Using tools like Weights & Biases for real-time monitoring is crucial. It allows you to catch issues early and make necessary adjustments.

3. **Verify Your Results**: Always test your fine-tuned model with various prompts to ensure it's behaving as expected.

4. **Resource Management**: Fine-tuning large language models is resource-intensive. Using techniques like QLoRA can help make the process more manageable on limited hardware.

5. **Iterate and Experiment**: Don't be afraid to try different configurations, learning rates, or even dataset preparations. Each iteration provides valuable insights.
