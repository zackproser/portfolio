import { createMetadata } from '@/utils/createMetadata';
import rawMetadata from './metadata.json';


export const metadata = createMetadata(rawMetadata);

## Table of contents

After using it for a few years now and building multiple production applications with it, I've had enough time to give the Vercel AI SDK a thorough review.

My biggest gripe is the usual for Next.js and Vercel docs: the examples are quite sparse, requiring some trial and error to implement more complex patterns.

What's most curious to me, though, is that more developers aren't talking about this toolkit - it's a surprisingly underappreciated gem in the AI ecosystem.

## What is the Vercel AI SDK?

At its core, the Vercel AI SDK was Vercel scooping everyone on the GenAI party while most people were still snoozing.

The Vercel AI SDK is a piece of technology, but it's also a convention and contract: it standardizes methods for interacting with LLMs that are abstracted across providers - it really requires only a few character change in a pull request to swap from Anthropic's Claude to the latest Google Gemini model without changing your prompt.

The result is that, once you learn to use the SDK, you can rapidly build high-quality GenAI applications on Vercel, which is exactly what Vercel intends.

## Latest features in AI SDK 4.2

Let's check out the latest additions to version 4.2 of the SDK. Model Context Protocol has been making the rounds lately, so it's not surprising to see first class support for it.

### Reasoning support

Reasoning models like Anthropic's Claude 3.7 Sonnet and DeepSeek R1 can now work seamlessly through the AI SDK, with access to their reasoning tokens:

```javascript
const { text, reasoning } = await generateText({
  model: anthropic('claude-3-7-sonnet-20250219'),
  prompt: 'How many people will live in the world in 2040?',
});
```

### Model Context Protocol (MCP) clients

Connect to MCP servers for tools like GitHub, Slack, and Filesystem access:

```javascript
const mcpClient = await createMCPClient({
  transport: {
    type: 'sse',
    url: 'https://my-server.com/sse',
  },
});

const response = await generateText({
  model: openai('gpt-4o'),
  tools: await mcpClient.tools(),
  prompt: 'Find products under $100',
});
```

### Message parts in useChat

Language models produce more than text, and the latest updates to the useChat hook allow you to easily access these other outputs:

```javascript
function Chat() {
  const { messages } = useChat();
  return (
    <div>
      {messages.map(message => (
        message.parts.map((part, i) => {
          switch (part.type) {
            case "text": return <p key={i}>{part.text}</p>;
            case "source": return <p key={i}>{part.source.url}</p>;
            case "reasoning": return <div key={i}>{part.reasoning}</div>;
            case "tool-invocation": return <div key={i}>{part.toolInvocation.toolName}</div>;
            case "file": return <img key={i} src={`data:${part.mimeType};base64,${part.data}`} />;
          }
        })
      ))}
    </div>
  );
}
```

## Infrastructure optimized for AI

The Vercel AI SDK isn't just a convenience layer - it's built on infrastructure specifically optimized for AI workloads. When you use this toolkit, you're leveraging Vercel's work to:

<ul>
  <li>Handle streaming responses efficiently through their Edge Functions</li>
  <li>Optimize memory usage during large model responses</li>
  <li>Automatically scale to handle concurrent AI requests</li>
  <li>Minimize latency between model providers and your frontend</li>
</ul>

This means your AI features don't just work - they work with production-grade performance out of the box.

## Instant swaps between models

One of the biggest wins is how trivial it is to swap between different LLMs, or even different providers:

```javascript
// Using OpenAI
const { text } = await generateText({
  model: openai("gpt-4o"),
  prompt: "What is love?"
});

// Switch to Anthropic with minimal changes
const { text } = await generateText({
  model: anthropic("claude-3-5-sonnet-latest"),
  prompt: "What is love?"
});
```

That ability to multiplex across providers is huge, and is something that many other platforms simply haven't caught up with yet.

## Quick prototyping at scale

The AI SDK offers a clean, consistent API for text generation and beyond:

```javascript
// Create a streaming text completion with minimal configuration
const textStream = await streamText({
  model: "openai/gpt-4o",  // Provider/model format makes switching easy
  prompt: "Explain the benefits of streaming responses.",
  // Additional parameters like temperature, maxTokens can be added here
});

// The SDK handles all the complexity of streaming the response to the client
res.send(textStream);
```

And for streaming structured objects:

```javascript
const objectStream = await streamObject({
  model: "openai/gpt-4o",
  prompt: "Suggest some structured data for product recommendations",
  // Example JSON schema for the object
  schema: {
    type: "object",
    properties: {
      item: { type: "string" },
      description: { type: "string" },
      price: { type: "number" }
    },
    required: ["item", "description", "price"]
  }
});
```

## Conclusion

The Vercel AI SDK is a powerful tool for building production-ready AI applications. Its ability to abstract away provider differences, handle streaming efficiently, and provide a consistent API makes it an excellent choice for developers looking to integrate AI into their applications.