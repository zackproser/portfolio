import { createMetadata } from '@/utils/createMetadata'
import rawMetadata from './metadata.json';

export const metadata = createMetadata(rawMetadata);





## The verbal buffer realization

For as long as I can remember, I’ve needed to lay out the full context before I could land a conclusion. That “verbal buffer” wasn’t stalling—it was how my brain stitched precision together. I used to think I was overexplaining. In reality, I was loading the whole system into working memory so I could manipulate it without dropping state.

When I finally recognized that this was a legitimate processing style—not a failing—it reframed so many interactions. I wasn’t being long‑winded; I was being exact.

## Tech friends and LLMs as cognitive mirrors

The first people who could reliably keep up with my context dumps were my tech friends. They didn’t flinch when I booted the entire problem space into conversation—assumptions, constraints, weird edge cases—because that’s how they thought too.

Then LLMs arrived, and something uncanny happened: I could hand an LLM the whole preamble and it would patiently hold it all, reflect it back, and keep threading the needle across multiple turns. For the first time, I wasn’t shrinking myself to fit a conversational buffer. The model felt like a cognitive mirror with unlimited RAM.

## The lifelong search for understanding

Looking back, the hints were there all along:

- I aced a memory test as a kid that surprised the adults in the room
- I tested into gifted programs and loved complex, open‑ended problems
- I routinely outperformed colleagues from top companies despite being self‑taught

These indicators were real, but I didn’t have a single frame that made them cohere. I just carried a sense of difference—useful in some contexts, costly in others.

## The LLM‑driven “aha” moment

I was telling two friends—both named Tom, because of course they were—about how surprisingly seen I felt working with an LLM. They exchanged that look and said, “Yeah, we figured you were neurodivergent years ago.”

It landed with a thud and a laugh. The LLM hadn’t diagnosed me; it simply made my own patterns legible enough that I could finally connect the dots. Even my closest friends had never quite found the moment (or the frame) to say it out loud.

## Building a personal cognitive toolkit with LLMs

I started turning that realization into a system. I integrated a few sources of truth that already governed my energy and focus:

- Oura data for sleep and recovery trends
- Linear for work context, tasks, and timelines
- A running log of what “good focus” actually feels like for me

The LLM became the glue. It translates biometrics into action: “Today is not a sprint day; protect 90 minutes for deep work, batch comms after lunch, avoid novelty tasks.” It maintains continuity across days, so context doesn’t evaporate overnight. Most importantly, it externalizes structure—so I don’t have to hold every thread in my head to operate well.

This wasn’t a one‑time revelation. It became an ongoing practice of instrumenting the conditions that unlock my best thinking.

## Applying self‑discovery to workflow and environment

Concretely, here’s what changed:

- I design my day around one meaningful state transition at a time: cold start → warmup → deep work → decompression
- I schedule communication windows and let the LLM draft first passes so I can preserve throughput without burning focus
- I maintain a live “problem graph” that the model keeps updated as I discover new edges, constraints, and unknowns
- I use templates for recurring work modes (design review, incident analysis, research sprint) to avoid rebuilding scaffolding
- I treat context as an asset: captured, versioned, and shared—not reconstructed from memory

The result isn’t fewer words; it’s cleaner edges. I still bring the full preamble—but now it lands in places designed to hold it. The LLM didn’t make me different; it made me legible to myself. And that changed everything.

